from fastapi import FastAPI, HTTPException
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
import uvicorn
import requests
import json
import random
import os
from typing import List, Optional, Dict, Any

app = FastAPI()

# Mount Debug Frontend
if not os.path.exists("debug_frontend"):
    os.makedirs("debug_frontend", exist_ok=True)
app.mount("/debug", StaticFiles(directory="debug_frontend", html=True), name="debug")

# è¨­å®š
OLLAMA_URL = "http://localhost:11434/api/generate"
MODEL_NAME = "llama3.1" # RTX 5080 (16GB VRAM) æ¨å¥¨

# ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«
class PlayerData(BaseModel):
    name: str
    location: Dict[str, float]
    tags: Dict[str, List[str]]

class ChatData(BaseModel):
    sender: str
    message: str

class ReportData(BaseModel):
    players: List[PlayerData]
    chats: List[ChatData]
    events: List[Dict[str, Any]] = [] # New: Receive events like death, item use


class DiscordReportData(BaseModel):
    discord_user_id: int
    text: str
    t0: float
    t1: float

# --- Voxel Sensor Models ---
class PlayerInfo(BaseModel):
    name: str
    pos: dict
    rot: Optional[dict] = None
    dimension: str

class VoxelSnapshot(BaseModel):
    player: PlayerInfo
    origin: dict
    radius: int
    halfHeight: int
    width: int
    height: int
    grid: List[int]

@app.post("/v1/mc/state")
def receive_state(snapshot: VoxelSnapshot):
    """ãƒã‚¤ã‚¯ãƒ©ã‹ã‚‰ã®è¦–ç•Œãƒ‡ãƒ¼ã‚¿(Voxel)ã‚’å—ã‘å–ã‚‹"""
    global latest_voxel_snapshot
    
    # Update global state for visualizer
    latest_voxel_snapshot = snapshot.dict()
    # Add path if available from brain (Mock for now or extract)
    # If we want to show the path *AI planned*, we should grab it from Brain.
    from parkour_brain import brain
    # Ideally brain updates its internal state when we call update_state below.
    # But path is calculated on 'get_next_action' or we can store last path.
    
    # Save to file for debug/visualization (Legacy)
    with open("latest_voxel.json", "w") as f:
        f.write(snapshot.json())

    # --- Parkour Logic (Simplified for now) ---
    from parkour_brain import brain
    
    # 1. Update Brain
    brain.update_state(snapshot.dict())
    
    return {"ok": True}

latest_voxel_snapshot = None

class GameEvent(BaseModel):
    type: str
    victim: str
    attacker: str
    timestamp: float

@app.post("/v1/mc/events")
def receive_event(evt: GameEvent):
    """ãƒã‚¤ã‚¯ãƒ©ã‹ã‚‰ã®ã‚¤ãƒ™ãƒ³ãƒˆå—ä¿¡"""
    if evt.type == "hit":
        print(f"ğŸ”¥ {evt.victim} was hit by {evt.attacker}!")
        # Update Brain Target
        from parkour_brain import brain
        brain.set_target_player(evt.attacker)
        
    return {"status": "ok"}

class UnmuteRequest(BaseModel):
    mcName: str

@app.post("/v1/discord/unmute")
def request_unmute(req: UnmuteRequest):
    """Ghost Modeã‹ã‚‰ã®ãƒŸãƒ¥ãƒ¼ãƒˆè§£é™¤ãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    # Simply fire a 'speak' event or specialized unmute event that bot polls
    # We reuse 'discord_report' queue logic?
    # Or create a new event type in the shared queue which bot polls via /pull
    
    # We append to 'events' queue that bot.py polls.
    # Where is that queue stored?
    # server.py doesn't seem to have a persistent event queue for Discord polling in the snippet I saw?
    # Let's check 'ChatData' or 'ReportData'?
    # Ah, 'command_queue' is for MC.
    # We need a queue for Discord.
    
    # Let's add it to a global 'discord_events' list if not exists, or just print for now if bot.py polls logs (unlikely).
    # Re-reading bot.py (Step 984), it polls POST_BASE/v1/discord/pull.
    # Let's check server.py endpoint for /pull.
    # If not found, I need to add it.
    
    global discord_events
    discord_events.append({
        "type": "mute", # Using 'mute' with 'target' to force unmute?
        # unique event for unmuting
        "type": "unmute_request",
        "mc_name": req.mcName
    })
    return {"status": "ok"}

discord_events = []

@app.post("/v1/discord/pull")
def pull_discord_events():
    global discord_events
    events = discord_events[:]
    discord_events = []
    # return events wrapped
    return {"events": events}

def get_latest_voxel():
    if latest_voxel_snapshot is None:
        return {"error": "no data"}
    
    # Inject current brain path if available
    from parkour_brain import brain
    data = latest_voxel_snapshot.copy()
    
    # Add debug info
    if brain.target_pos:
        data["DEBUG_target"] = brain.target_pos
    if brain.path:
         data["path"] = brain.path # Ensure brain path is list of dicts or convertible

    return data
    
@app.post("/v1/mc/next_move")
def get_next_move(player_name: str = "Bot"): 
    """Botã®æ¬¡ã®å‹•ä½œã‚’æ±ºå®šã—ã¦è¿”ã™ (High-Frequency Polling)"""
    from parkour_brain import brain
    
    if latest_voxel_snapshot:
        brain.update_state(latest_voxel_snapshot)
        
        # --- Priority 1: Chase (Target Player) ---
        target_rel = None
        has_target = False
        
        from game_master import gm
        
        # Check active CHASE target
        if brain.target_player:
            target_p = next((p for p in gm.state.players if p.name == brain.target_player), None)
            if target_p:
                dist = _calc_dist(latest_voxel_snapshot["origin"], target_p.location)
                if dist > 30: 
                    print(f"Chase: Lost target (too far {dist:.1f})")
                    brain.target_player = None
                elif dist < 1.5:
                    print(f"Chase: Caught up!")
                    # Attack Logic could go here (send 'attack' command?)
                else:
                    target_rel = _calc_rel(latest_voxel_snapshot["origin"], target_p.location)
                    has_target = True
            else:
                brain.target_player = None
        
        # --- Priority 2: Observe (Being Watched) ---
        # If someone is looking at us, stare back and freeze (fear factor)
        if not has_target:
            my_pos = latest_voxel_snapshot["origin"]
            
            for p_name, p_state in gm.state.players.items():
                if p_name == player_name: continue
                if not p_state.is_alive: continue
                # Skip spectators
                if "spectator" in p_state.role or "ghost" in p_state.tags: continue

                # Check if looking at me
                # Vector from Them -> Me
                dx = my_pos["x"] - p_state.location["x"]
                dz = my_pos["z"] - p_state.location["z"]
                dist = (dx**2 + dz**2)**0.5
                
                if dist < 20: # Only care if close enough
                     # Normalize direction to me
                     dir_to_me = {"x": dx/dist, "z": dz/dist}
                     
                     # Their view vector (from rotation y/yaw)
                     # Yaw in MC: 0=South(+Z), 90=West(-X), 180=North(-Z), -90=East(+X)
                     # Convert to Rad
                     import math
                     yaw_rad = (p_state.rotation["y"] + 90) * (math.pi / 180)
                     # View Vector (2D XZ)
                     view_x = math.cos(yaw_rad)
                     view_z = math.sin(yaw_rad)
                     
                     # Dot Product
                     dot = dir_to_me["x"] * view_x + dir_to_me["z"] * view_z
                     
                     # If dot > 0.9 (approx 25 deg cone), they are looking at us
                     if dot > 0.9:
                         # Reaction: Stare back (Turn to them)
                         # Set target to them, but maybe DON'T move?
                         # For now, let's just turn to face them.
                         # ParkourBrain.get_next_action normally moves toward target.
                         # We might need a special action "scan" or "idle_face".
                         # For MVP: Just set them as target (Bot will walk to them slowly/creepy).
                         # Or verify logic: if we set target, brain pathfinds.
                         # Let's say "If watched, approach slowly" (Creepy).
                         target_rel = _calc_rel(my_pos, p_state.location)
                         has_target = True
                         # print(f"Observe: {p_name} is watching! Staring back.")
                         break

        # --- Priority 3: Group Up (If no chase target) ---
        if not has_target:
            # Find nearest living player to stick with
            nearest = None
            min_d = 999
            my_pos = latest_voxel_snapshot["origin"]
            
            for p_name, p_state in gm.state.players.items():
                if p_name == player_name: continue
                if not p_state.is_alive: continue
                # Skip spectators/ghosts? 
                if "spectator" in p_state.role or "ghost" in p_state.tags: continue # Simple check
                
                d = _calc_dist(my_pos, p_state.location)
                if d < min_d:
                    min_d = d
                    nearest = p_state
            
            # Logic: If isolated (> 8 blocks), move closer. If too close (< 3), stop/back up.
            if nearest and min_d > 5.0 and min_d < 50.0:
                 # print(f"Group: Moving to {nearest.name} ({min_d:.1f}m)")
                 target_rel = _calc_rel(my_pos, nearest.location)
                 has_target = True
        
        # --- Priority 3: Wander (Handled by Brain fallback) ---
        
        cmd = brain.get_next_action(target_rel)
        return cmd
        
    return {"type": "idle"}

def _calc_dist(p1, p2):
    return ((p1["x"]-p2["x"])**2 + (p1["z"]-p2["z"])**2)**0.5

def _calc_rel(from_pos, to_pos):
    return (int(to_pos["x"] - from_pos["x"]), 
            int(to_pos["y"] - from_pos["y"]), 
            int(to_pos["z"] - from_pos["z"]))

# ã‚²ãƒ¼ãƒ çŠ¶æ…‹ã¨ã‚³ãƒãƒ³ãƒ‰ã‚­ãƒ¥ãƒ¼
game_state = {
    "chat_history": [],
    "players": []
}
command_queue: List[Dict[str, Any]] = []
discord_queue: List[Dict[str, Any]] = []

# LLM Config (LM Studio / Ollama)
LLM_API_BASE = os.getenv("LLM_API_BASE", "http://127.0.0.1:1234/v1")
LLM_MODEL = os.getenv("LLM_MODEL", "local-model") # LM Studio often ignores model name or uses loaded model

def call_llm(prompt: str) -> Optional[str]:
    """LM Studio (OpenAI Compatible) ã«ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’é€ã‚‹"""
    try:
        # system prompt + user prompt
        messages = [
            {"role": "system", "content": "You are a helpful Minecraft AI assistant. Reply in JSON only."},
            {"role": "user", "content": prompt}
        ]
        
        # print(f"DEBUG: Calling LLM at {LLM_API_BASE}...")
        
        # Synchronous implementation for simplicity in this thread, or use httpx inside async route
        # Since this is called from async 'think_and_queue', we should ideally use async logic or run_in_executor.
        # But 'think_and_queue' in this file (checked line 449) is async.
        # Let's use httpx.post directly if we can, or requests.
        # Just going with persistent client or simple one-off.
        
        import requests
        resp = requests.post(
            f"{LLM_API_BASE}/chat/completions",
            json={
                "model": LLM_MODEL,
                "messages": messages,
                "temperature": 0.7,
                "max_tokens": 300
            },
            timeout=10 # Fast timeout
        )
        
        if resp.status_code == 200:
            data = resp.json()
            content = data["choices"][0]["message"]["content"]
            # Clean up potential markdown code blocks
            if "```json" in content:
                content = content.split("```json")[1].split("```")[0].strip()
            elif "```" in content:
                content = content.split("```")[1].split("```")[0].strip()
            return content
        else:
            print(f"LLM Error: {resp.status_code} {resp.text}")
            return None
            
    except Exception as e:
        print(f"LLM Exception: {e}")
        return None


@app.post("/v1/discord/report")
async def discord_report(data: DiscordReportData):
    """Discordã‹ã‚‰ã®éŸ³å£°èªè­˜çµæœã‚’å—ã‘å–ã‚‹"""
    global game_state
    
    print(f"[Discord Voice] {data.discord_user_id}: {data.text}")
    
    chat_entry = {
        "sender": f"Discord:{data.discord_user_id}",
        "message": data.text
    }
    game_state["chat_history"].append(chat_entry)
    
    # AIã«æ€è€ƒã•ã›ã‚‹
    await think_and_queue()
    
    return {"status": "ok"}

@app.post("/v1/report")
async def report(data: ReportData):
    """ãƒã‚¤ã‚¯ãƒ©ã‹ã‚‰ã®çŠ¶æ³å ±å‘Šã‚’å—ã‘å–ã‚‹"""
    global game_state, discord_queue
    
    # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ä½ç½®æ›´æ–°
    game_state["players"] = [p.dict() for p in data.players]
    
    # ãƒãƒ£ãƒƒãƒˆå±¥æ­´æ›´æ–° & èª­ã¿ä¸Šã’
    for chat in data.chats:
        print(f"Chat received: {chat.sender}: {chat.message}")
        game_state["chat_history"].append(chat.dict())
        
        # Discordã§èª­ã¿ä¸Šã’ (TTS)
        discord_queue.append({
            "type": "speak",
            "text": f"{chat.sender}ã€Œ{chat.message}ã€"
        })
        
        # ãƒãƒ£ãƒƒãƒˆã‚’å—ã‘å–ã£ãŸã‚‰æ€è€ƒã™ã‚‹
        await think_and_queue()

    # Process events through GameMaster (Mocking event extraction from report)
    # The actual implementation needs GameMaster integration here similar to previous plan
    # But for now, let's just show where Discord events would be handled if GM returns them.
    # In a real scenario, we'd extract events from data and pass to GM.
    # Since I don't have the full GM integration in this file yet (it was overwritten or missed in previous steps),
    # I will re-add the GM integration properly.
    
    from game_master import gm
    
    # Mock extracting events from data (needs client side support to send 'events' list)
    # For now, let's assume ReportData has an 'events' field in future, or we parse chat/actions.
    # To demonstrate the logic:
    
    minecraft_commands = []
    
    # Example: If GM returned commands, we separate them
    # events = data.events (NEED TO ADD TO MODEL)
    # for cmd in gm.process_event(event):
    #    if cmd.get("type") == "discord_event":
    #        discord_queue.append(cmd["event"])
    #    else:
    #        minecraft_commands.append(cmd)
            
    return {"status": "ok", "commands": minecraft_commands}

@app.post("/v1/discord/pull")
async def discord_pull():
    """Discord Botã‹ã‚‰ã®ãƒãƒ¼ãƒªãƒ³ã‚°ã«å¯¾ã—ã€æºœã¾ã£ã¦ã„ã‚‹ç™ºè¨€ã‚­ãƒ¥ãƒ¼ã‚’è¿”ã™"""
    global discord_queue
    
    if not discord_queue:
        return {"events": []}
    
    events_to_send = discord_queue.copy()
    discord_queue = []
    
    return {"events": events_to_send}

    return {"events": events_to_send}

class CommandRequest(BaseModel):
    type: str
    player: str
    target: Optional[str] = None

@app.post("/v1/mc/command_request")
async def command_request(cmd: CommandRequest):
    """Discord Botã‹ã‚‰ã®ã‚³ãƒãƒ³ãƒ‰ã‚­ãƒ¥ãƒ¼è¿½åŠ ãƒªã‚¯ã‚¨ã‚¹ãƒˆ"""
    # Simply push to command_queue for Minecraft to pick up
    target_action = {
        "action": cmd.type,
        "player": cmd.player,
        "target": cmd.target
    }
    # type="camera_control", target="next" or "stop"
    
    command_queue.append(target_action)
    return {"status": "queued"}

@app.get("/v1/mc/commands")
def poll_commands():
    """Minecraftå´ãŒæºœã¾ã£ã¦ã„ã‚‹ã‚³ãƒãƒ³ãƒ‰ã‚’å–ã‚Šã«æ¥ã‚‹"""
    global command_queue
    if not command_queue:
        return {"commands": []}
    
    cmds = command_queue.copy()
    command_queue = [] # Clear
    return {"commands": cmds}

class GameConfig(BaseModel):
    roles: Dict[str, int]

@app.post("/v1/game/start")
async def start_game():
    """Discordç­‰ã‹ã‚‰ã‚²ãƒ¼ãƒ é–‹å§‹ã‚’ãƒˆãƒªã‚¬ãƒ¼ã™ã‚‹"""
    from game_master import gm
    # Current config (can be stored in game_state)
    # For now, default or last config
    config = game_state.get("role_config", {"werewolf": 1})
    gm.start_game(config)
    
    # Send start message to Discord
    discord_queue.append({
        "type": "message",
        "channel_id": "DEFAULT",
        "content": "**ã‚²ãƒ¼ãƒ ã‚’é–‹å§‹ã—ã¾ã—ãŸï¼** ğŸ®"
    })
    discord_queue.append({
        "type": "speak",
        "text": "ã‚²ãƒ¼ãƒ ã‚’é–‹å§‹ã—ã¾ã™ã€‚å„ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã«å½¹è·ã‚’é…å¸ƒã—ã¾ã—ãŸã€‚"
    })
    
    return {"status": "started", "config": config}

@app.post("/v1/game/config")
async def config_game(config: GameConfig):
    """å½¹è·æ§‹æˆã‚’è¨­å®šã™ã‚‹"""
    game_state["role_config"] = config.roles
    print(f"Game Config Updated: {config.roles}")
    return {"status": "updated", "config": config.roles}

class AiModeConfig(BaseModel):
    mode: str # 'player' or 'gm'

@app.post("/v1/game/ai_mode")
async def set_ai_mode(config: AiModeConfig):
    game_state["ai_mode"] = config.mode
    print(f"AI Mode switched to: {config.mode}")
    return {"status": "updated", "mode": config.mode}

async def think_and_queue():
    """AIã«æ€è€ƒã•ã›ã€çµæœã‚’ã‚³ãƒãƒ³ãƒ‰ã‚­ãƒ¥ãƒ¼(ãƒã‚¤ã‚¯ãƒ©&Discord)ã«è¿½åŠ ã™ã‚‹"""
    print("AI Thinking...")
    
    mode = game_state.get("ai_mode", "player") # 'player' or 'gm'

    base_info = f"""
    ç¾åœ¨ã®çŠ¶æ³:
    - ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ä¸€è¦§: {json.dumps(game_state['players'])}
    - ç›´è¿‘ã®ãƒãƒ£ãƒƒãƒˆ: {json.dumps(game_state['chat_history'][-5:])}
    """

    if mode == "gm":
        prompt = f"""
        ã‚ãªãŸã¯Minecraftäººç‹¼ã‚²ãƒ¼ãƒ ã®ã€Œã‚²ãƒ¼ãƒ ãƒã‚¹ã‚¿ãƒ¼(GM)ã€å…¼ã€Œå®Ÿæ³è€…ã€ã§ã™ã€‚
        {base_info}
        
        å½¹å‰²:
        - ã‚²ãƒ¼ãƒ ã®é€²è¡ŒçŠ¶æ³ã‚’æŠŠæ¡ã—ã€ç››ã‚Šä¸Šã’ã‚‹å®Ÿæ³ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚
        - ãƒ«ãƒ¼ãƒ«ã®èª¬æ˜ã‚„ã€æ€ªã—ã„è¡Œå‹•ã¸ã®ãƒ„ãƒƒã‚³ãƒŸã‚’å…¥ã‚Œã¦ãã ã•ã„ã€‚
        - ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¨ã—ã¦ã¯è¡Œå‹•ã—ã¾ã›ã‚“ (move/attackã¯åŸºæœ¬ idle)ã€‚
        
        æ¬¡ã®JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§è¡Œå‹•ã‚’æ±ºå®šã—ã¦ãã ã•ã„:
        {{
            "action": "chat" | "idle",
            "message": "å®Ÿæ³ã‚³ãƒ¡ãƒ³ãƒˆ",
            "reason": "ã‚³ãƒ¡ãƒ³ãƒˆã®ç†ç”±"
        }}
        """
    else:
        # Player Mode (Default)
        prompt = f"""
        ã‚ãªãŸã¯Minecraftã®äººç‹¼ã‚²ãƒ¼ãƒ ã®ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼(AIãƒœãƒƒãƒˆ)ã§ã™ã€‚
        {base_info}
        
        ã‚¿ã‚°ã®è¦‹æ–¹:
        - pub: å…¬é–‹æƒ…å ± (å…¨å“¡ãŒè¦‹ãˆã‚‹çŠ¶æ…‹)
        - sec: ç§˜åŒ¿æƒ…å ± (å½¹è·ãªã©ã€ã‚ãªãŸã ã‘ãŒçŸ¥ã£ã¦ã„ã‚‹æƒ…å ±)
        
        ã‚ãªãŸã¯ã€Œæ‘äººã€ã¨ã—ã¦æŒ¯ã‚‹èˆã£ã¦ãã ã•ã„ã€‚
        æ€ªã—ã„ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒã„ã‚Œã°æ”»æ’ƒã—ã€ãƒãƒ£ãƒƒãƒˆã§ä¼šè©±ã—ã¦ãã ã•ã„ã€‚
        
        æ¬¡ã®JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§è¡Œå‹•ã‚’æ±ºå®šã—ã¦ãã ã•ã„:
        {{
            "action": "move" | "attack" | "chat" | "idle",
            "target": "ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼å (attack/moveã®å ´åˆ)",
            "message": "ãƒãƒ£ãƒƒãƒˆå†…å®¹ (chatã®å ´åˆ)",
            "reason": "è¡Œå‹•ã®ç†ç”±"
        }}
        """
        
    prompt += "\nå¿…ãšJSONã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"
    
    llm_response = call_llm(prompt)
    if llm_response:
        try:
            action = json.loads(llm_response)
            print(f"AI Decided: {action}")
            
            # ãƒã‚¤ã‚¯ãƒ©ç”¨ã‚­ãƒ¥ãƒ¼ã«è¿½åŠ 
            command_queue.append(action)
            
            # ç™ºè¨€(chat)ãªã‚‰Discordç”¨ã‚­ãƒ¥ãƒ¼ã«ã‚‚è¿½åŠ ã—ã¦åŒæœŸã•ã›ã‚‹
            if action.get("action") == "chat" and action.get("message"):
                discord_queue.append({
                    "type": "speak",
                    "text": action["message"]
                })
                
        except:
            print("JSON Parse Error")

if __name__ == "__main__":
    print(f"Starting FastAPI Server on port 8082 (Model: {MODEL_NAME})...")
    uvicorn.run(app, host="0.0.0.0", port=8082)
